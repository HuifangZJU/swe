# 对数损失

对数损失是用于最大似然估计的。
一组参数在一堆数据下的似然值，等于每一条数据的概率之积。
而损失函数一般是每条数据的损失之和，为了把积变为和，就取了对数。
再加个负号是为了让最大似然值和最小损失对应起来。


对数损失是逻辑回归推到得到的产物。有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。

1. 平方损失函数是线性回归在假设样本是高斯分布的条件下推导得到的（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是[中心极限定理](http://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Central_limit_theorem)）。
2. 逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数等等。逻辑回归并没有极大化似然函数求极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即$max F(y, f(x)) ---> min -F(y, f(x)))$。从损失函数的视角来看，它就成了对数损失函数了。
对数损失函数的标准形式：
$$L(Y,P(Y|X)) = -logP(Y|X)$$

刚刚说到，对数是用来做极大似然估计的，图的是计算方便，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，就是利用已知的样本分布，找到最有可能导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大）。而log是单调递增函数，所以logP(Y|X)也会达到最大值，因此前面加符号之后，最大化P(Y|X)就等同于最小化L。

逻辑回归的P(Y=y|x)表达式如下：
$$P(Y=y|x) = \frac{1}{1+exp(-yf(x))}$$
将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：
$$L(y, P(Y=y|x)) = log(1+exp(-yf(x)))$$
逻辑回归最后得到的目标式子如下：
$$J(\theta) = -\frac1m[\Sigma_{i=1}^{m}y^{(i)}logh_\theta (x^{(i)}) + (1-y^{(i)})log(1-h_\theta (x^{(i)}))]$$

如果是二分类的话，则m取值为2，如果是多分类，m就相应的类别个数。

这里需要说明一下：之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种错觉了。
